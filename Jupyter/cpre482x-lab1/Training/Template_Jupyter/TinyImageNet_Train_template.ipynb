{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VWQY7_ZTP5aH"
   },
   "outputs": [],
   "source": [
    "# import the necessary libraries\n",
    "import pathlib\n",
    "import numpy as np\n",
    "import os\n",
    "#import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-12 13:10:11.158010: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-12 13:10:13.418921: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1\n",
      "2021-09-12 13:10:13.453766: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-09-12 13:10:13.454110: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \n",
      "pciBusID: 0000:02:02.0 name: GRID RTX8000-2Q computeCapability: 7.5\n",
      "coreClock: 1.77GHz coreCount: 72 deviceMemorySize: 2.00GiB deviceMemoryBandwidth: 625.94GiB/s\n",
      "2021-09-12 13:10:13.454149: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
      "2021-09-12 13:10:13.501957: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.10\n",
      "2021-09-12 13:10:13.502073: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.10\n",
      "2021-09-12 13:10:13.524539: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\n",
      "2021-09-12 13:10:13.564835: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\n",
      "2021-09-12 13:10:13.607319: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\n",
      "2021-09-12 13:10:13.626600: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.10\n",
      "2021-09-12 13:10:13.710926: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.7\n",
      "2021-09-12 13:10:13.711174: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-09-12 13:10:13.711640: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysF"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "os.environ['TF_XLA_FLAGS'] = '--tf_xla_enable_xla_devices'\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL']='2'\n",
    "os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version:  2.4.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "S had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-09-12 13:10:13.711852: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'device_name' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_21896/706599607.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_visible_devices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'GPU'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mdevice_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgpu_device_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdevice_name\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Running on CPU'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'device_name' is not defined"
     ]
    }
   ],
   "source": [
    "# # Enable or disable GPU\n",
    "ENABLE_GPU = False\n",
    "tf.debugging.set_log_device_placement(True)\n",
    "print(\"TensorFlow version: \", tf.__version__)\n",
    "devices = tf.config.list_physical_devices(device_type=None)\n",
    "if ENABLE_GPU:\n",
    "    tf.config.set_visible_devices([], 'GPU')\n",
    "    device_name = tf.test.gpu_device_name()\n",
    "if not device_name:\n",
    "    print('Running on CPU')\n",
    "else:\n",
    "    print('Found GPU at: {}'.format(device_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vrz63rys1qNr"
   },
   "outputs": [],
   "source": [
    "#define path to the dataset\n",
    "path = pathlib.Path('/local/tiny-imagenet-200')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kAw5faJHQT03"
   },
   "outputs": [],
   "source": [
    "# First load wnids\n",
    "with open(os.path.join(path, 'wnids.txt'), 'r') as f:\n",
    "  wnids = [x.strip() for x in f]\n",
    "\n",
    "# Map wnids to integer labels\n",
    "wnid_to_label = {wnid: i for i, wnid in enumerate(wnids)}\n",
    "label_to_wnid = {v: k for k, v in wnid_to_label.items()}\n",
    "\n",
    "\n",
    "# Use words.txt to get names for each class\n",
    "with open(os.path.join(path, 'words.txt'), 'r') as f:\n",
    "  wnid_to_words = dict(line.split('\\t') for line in f)\n",
    "  for wnid, words in wnid_to_words.items():\n",
    "      wnid_to_words[wnid] = [w.strip() for w in words.split(',')]\n",
    "class_names = [wnid_to_words[wnid] for wnid in wnids]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "z_5SvXQF0Tz6"
   },
   "outputs": [],
   "source": [
    "# Function to load training and validation data\n",
    "\n",
    "# Import necessary packages\n",
    "from __future__ import print_function\n",
    "from builtins import range\n",
    "from six.moves import cPickle as pickle\n",
    "import numpy as np\n",
    "import os\n",
    "import platform\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "from imageio import imread\n",
    "from imageio import imsave\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Function for loading the tiny imagenet data\n",
    "def load_tiny_imagenet(path, is_training=True, dtype=np.float32, subtract_mean=True, debug=False, debug_nclass=3):\n",
    "    \"\"\"\n",
    "    Load TinyImageNet. Each of TinyImageNet-100-A, TinyImageNet-100-B, and\n",
    "    TinyImageNet-200 have the same directory structure, so this can be used\n",
    "    to load any of them.\n",
    "\n",
    "    Note: The original implementation loaded data as NCHW, I (tyler) changed it to NHWC\n",
    "\n",
    "    Inputs:\n",
    "    - path: String giving path to the directory to load.\n",
    "    - is_training: If True, dont load testing data, if False, dont load training and val data\n",
    "        Note: Must always load training data in order to subtract_mean.\n",
    "    - dtype: numpy datatype used to load the data.\n",
    "    - subtract_mean: Whether to subtract the mean training image.\n",
    "    - debug: Whether or not to load a small number of classes for debugging\n",
    "\n",
    "    Returns: A dictionary with the following entries:\n",
    "    - class_names: A list where class_names[i] is a list of strings giving the\n",
    "      WordNet names for class i in the loaded dataset.\n",
    "    - X_train: (N_tr, 64, 64, 3) array of training images\n",
    "    - y_train: (N_tr,) array of training labels\n",
    "    - X_val: (N_val, 64, 64, 3) array of validation images\n",
    "    - y_val: (N_val,) array of validation labels\n",
    "    - X_test: (N_test, 64, 64, 3) array of testing images.\n",
    "    - y_test: (N_test,) array of test labels; if test labels are not available\n",
    "      (such as in student code) then y_test will be None.\n",
    "    - mean_image: (64, 64, 3) array giving mean training image\n",
    "    - label_to_wnid: dictionary with mapping from integer class label to wnid\n",
    "    \"\"\"\n",
    "    # First load wnids\n",
    "    with open(os.path.join(path, 'wnids.txt'), 'r') as f:\n",
    "        wnids = [x.strip() for x in f]\n",
    "\n",
    "    # Map wnids to integer labels\n",
    "    wnid_to_label = {wnid: i for i, wnid in enumerate(wnids)}\n",
    "    label_to_wnid = {v: k for k, v in wnid_to_label.items()}\n",
    "\n",
    "    # Use words.txt to get names for each class\n",
    "    with open(os.path.join(path, 'words.txt'), 'r') as f:\n",
    "        wnid_to_words = dict(line.split('\\t') for line in f)\n",
    "        for wnid, words in wnid_to_words.items():\n",
    "            wnid_to_words[wnid] = [w.strip() for w in words.split(',')]\n",
    "    class_names = [wnid_to_words[wnid] for wnid in wnids]\n",
    "\n",
    "    if debug:\n",
    "        print('Debug is on! Only loading %d / %d training classes.'\n",
    "                  % (debug_nclass, len(wnids)))\n",
    "\n",
    "    # Next load training data.\n",
    "    X_train, y_train = [], []\n",
    "    train_wnids = wnids[:debug_nclass] if debug else wnids\n",
    "    for i, wnid in tqdm(enumerate(train_wnids), total=len(train_wnids)):\n",
    "        # To figure out the filenames we need to open the boxes file\n",
    "        boxes_file = os.path.join(path, 'train', wnid, '%s_boxes.txt' % wnid)\n",
    "        with open(boxes_file, 'r') as f:\n",
    "            filenames = [x.split('\\t')[0] for x in f]\n",
    "        num_images = len(filenames)\n",
    "\n",
    "        X_train_block = np.zeros((num_images, 64, 64, 3), dtype=dtype)\n",
    "        y_train_block = wnid_to_label[wnid] * \\\n",
    "                        np.ones(num_images, dtype=np.int64)\n",
    "        for j, img_file in enumerate(filenames):\n",
    "            img_file = os.path.join(path, 'train', wnid, 'images', img_file)\n",
    "            img = imread(img_file)\n",
    "            if img.ndim == 2:   ## grayscale file\n",
    "                img.shape = (64, 64, 1)\n",
    "            X_train_block[j] = img\n",
    "        X_train.append(X_train_block)\n",
    "        y_train.append(y_train_block)\n",
    "\n",
    "    # We need to concatenate all training data\n",
    "    X_train = np.concatenate(X_train, axis=0)\n",
    "    y_train = np.concatenate(y_train, axis=0)\n",
    "\n",
    "    # Next load validation data\n",
    "    X_val, y_val = None, None\n",
    "    if is_training:\n",
    "        print('loading validation data')\n",
    "        with open(os.path.join(path, 'val', 'val_annotations.txt'), 'r') as f:\n",
    "            img_files = []\n",
    "            val_wnids = []\n",
    "            for line in f:\n",
    "                img_file, wnid = line.split('\\t')[:2]\n",
    "                img_files.append(img_file)\n",
    "                val_wnids.append(wnid)\n",
    "            num_val = len(img_files)\n",
    "            y_val = np.array([wnid_to_label[wnid] for wnid in val_wnids])\n",
    "            X_val = np.zeros((num_val, 64, 64, 3), dtype=dtype)\n",
    "            for i, img_file in tqdm(enumerate(img_files), total=len(img_files)):\n",
    "                img_file = os.path.join(path, 'val', 'images', img_file)\n",
    "                img = imread(img_file)\n",
    "                if img.ndim == 2:\n",
    "                    img.shape = (64, 64, 1)\n",
    "                X_val[i] = img\n",
    "\n",
    "    mean_image = None\n",
    "    if subtract_mean:\n",
    "        mean_image = X_train.mean(axis=0)\n",
    "        if is_training:\n",
    "            X_train -= mean_image[None]\n",
    "            X_val -= mean_image[None]\n",
    "        else:\n",
    "            X_test -= mean_image[None]\n",
    "\n",
    "    if not is_training:\n",
    "        X_train = None\n",
    "        y_train = None\n",
    "\n",
    "    return {\n",
    "      'class_names': class_names,\n",
    "      'X_train': X_train,\n",
    "      'y_train': y_train,\n",
    "      'X_val': X_val,\n",
    "      'y_val': y_val,\n",
    "      'mean_image': mean_image,\n",
    "      'label_to_wnid': label_to_wnid\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 205
    },
    "colab_type": "code",
    "id": "eZJ9EQPH2XF0",
    "outputId": "2459b192-8943-4281-f284-6879d56c15f6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Tiny-Imagenet Dataset for training and validation data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|████████████████████████▏                | 118/200 [00:34<00:22,  3.64it/s]"
     ]
    }
   ],
   "source": [
    "print (\"Loading Tiny-Imagenet Dataset for training and validation data\")\n",
    "dataset_train_val = load_tiny_imagenet(path, is_training = True, dtype=np.float32, subtract_mean=False)\n",
    "x_train = dataset_train_val[\"X_train\"] # 100000 images when each is a 64*64*3\n",
    "y_train = dataset_train_val[\"y_train\"] # 100000 elements\n",
    "x_val = dataset_train_val[\"X_val\"] # 10000 images when each is a 64*64*3\n",
    "y_val = dataset_train_val[\"y_val\"] # 10000 elements\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mKiramsCwe8h"
   },
   "outputs": [],
   "source": [
    "x_train=x_train/255.0\n",
    "x_val=x_val/255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-D7Vn0oEKbwl"
   },
   "outputs": [],
   "source": [
    "# one hot encode y data\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "num_classes = 200\n",
    "y_train = to_categorical(y_train, num_classes)\n",
    "y_val = to_categorical(y_val, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XBJA8zevI5Yb"
   },
   "outputs": [],
   "source": [
    "# Import required packages for training\n",
    "import datetime\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.losses import categorical_crossentropy\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.layers import Dense, Flatten, Conv2D, AveragePooling2D, MaxPooling2D, ZeroPadding2D,Convolution2D, Activation, Dropout \n",
    "from tensorflow.keras import datasets\n",
    "\n",
    "\n",
    "#from __future__ import absolute_import, division, print_function, unicode_literals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CA7NiZHDJ8WO"
   },
   "outputs": [],
   "source": [
    "# Place the logs in a timestamped subdirectory\n",
    "# This allows to easy select different training runs\n",
    "# In order not to overwrite some data, it is useful to have a name with a timestamp\n",
    "log_dir=\"C:/Temp/CPRE482X/Lab1/Files/Training/logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "# Specify the callback object\n",
    "# tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "# tf.keras.callback.TensorBoard ensures that logs are created and stored\n",
    "# We need to pass callback object to the fit method\n",
    "# The way to do this is by passing the list of callback objects, which is in our case just one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 571
    },
    "colab_type": "code",
    "id": "iUawHlDUOK97",
    "outputId": "47d1bf1b-e36d-43cc-fadc-9552bb696335"
   },
   "outputs": [],
   "source": [
    "# Basic CNN model\n",
    "model = Sequential()\n",
    "\n",
    "# conv1\n",
    "model.add(Conv2D(32, (5, 5), input_shape= (64, 64, 3),activation='relu'))\n",
    "model.add(Conv2D(32,(5,5),activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Conv2D(64,(3,3),activation='relu'))\n",
    "model.add(Conv2D(64,(3,3),activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Conv2D(64,(3,3),activation='relu'))\n",
    "model.add(Conv2D(128,(3,3),activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Flatten())\n",
    "\n",
    "# fc1\n",
    "model.add(Dense(256,activation='relu'))\n",
    "\n",
    "# fc2\n",
    "model.add(Dense(200,activation='softmax'))\n",
    "\n",
    "# sgd = SGD(lr=0.01, momentum=0.9)\n",
    "model.compile(loss='categorical_crossentropy' ,optimizer='adam',metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "qfzVH5hSVZbW",
    "outputId": "66d00118-f4c6-4302-b3b8-6920a9aea5a2"
   },
   "outputs": [],
   "source": [
    "# Using early stopping to monitor validation accuracy\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.EarlyStopping(\n",
    "        # Stop training when `val_loss` is no longer improving\n",
    "        monitor=\"val_loss\",\n",
    "        # \"no longer improving\" being defined as \"no better than 1e-2 less\"\n",
    "        min_delta=1e-2,\n",
    "        # \"no longer improving\" being further defined as \"for at least 2 epochs\"\n",
    "        patience=2,\n",
    "        verbose=1,\n",
    "    )\n",
    "#     tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1),\n",
    "    \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 386
    },
    "colab_type": "code",
    "id": "2qR-UdvFKDI7",
    "outputId": "b2cd11a5-6e41-44be-f540-de742af1fa69"
   },
   "outputs": [],
   "source": [
    "# Training on the tiny image dataset\n",
    "# Experiment here with the batch size, number of epochs etc as mentioned in the lab1 doc.\n",
    "epochs = [10, 20, 50]\n",
    "batch_sizes = [2, 8, 16]\n",
    "\n",
    "for batch_size in batch_sizes:\n",
    "    model.compile(loss='categorical_crossentropy' ,optimizer='adam',metrics=['accuracy'])\n",
    "    model.fit(x_train,y_train, validation_data=(x_val, y_val),batch_size=batch_size,callbacks=callbacks,\n",
    "              epochs=20)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#model.fit(x_train,y_train, validation_data=(x_val, y_val),batch_size=32,\n",
    "#          epochs=20)\n",
    "val_set_split = [0.5, 0.75]\n",
    "\n",
    "#model.fit_generator(x_train,y_train, validation_data=(x_val, y_val),batch_size=32,epochs=10)\n",
    "\n",
    "\n",
    "# model.fit(x_train,y_train, validation_split=0.3,batch_size=25,\n",
    "#           epochs=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in epochs:\n",
    "    model.compile(loss='categorical_crossentropy' ,optimizer='adam',metrics=['accuracy'])\n",
    "    model.fit(x_train,y_train, validation_data=(x_val, y_val),batch_size=256,callbacks=callbacks,\n",
    "              epochs=epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for val_split in val_set_split:\n",
    "    model.fit(x_train,y_train, validation_split=val_split,batch_size=25,\n",
    "          epochs=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "mhQnvMlaWZ16",
    "outputId": "59b288d6-b1f8-416d-b24d-786af0fab419"
   },
   "outputs": [],
   "source": [
    "# Save the cnn model\n",
    "model.save('CNN_TinyImageNet_trained.h5')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "TinyImagenet_Basic_CNN.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
